{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Import."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T22:18:41.145057Z",
     "start_time": "2020-12-05T22:18:41.119995Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "import traceback\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "import nltk\n",
    "\n",
    "import pyconll\n",
    "\n",
    "from nltk.tag import DefaultTagger\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tag import BigramTagger, TrigramTagger\n",
    "from nltk.tag import RegexpTagger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Тема “Создание признакового пространства”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Используем предобработанные в рамках 1-ого домашнего задания датасет combine_df_prepocessed.pkl. Используем столбец 'clean_tweet'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T21:33:07.315801Z",
     "start_time": "2020-12-05T21:33:05.905007Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_source</th>\n",
       "      <th>tweet_</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_token_</th>\n",
       "      <th>tweet_token_filtered</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>when father is dysfunctional and is so selfish...</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>when father is dysfunctional and is so selfish...</td>\n",
       "      <td>[when, father, is, dysfunctional, and, is, so,...</td>\n",
       "      <td>[when, father, is, dysfunctional, and, is, so,...</td>\n",
       "      <td>[father, dysfunctional, selfish, drags, kids, ...</td>\n",
       "      <td>[father, dysfunct, selfish, drag, kid, dysfunc...</td>\n",
       "      <td>[father, dysfunctional, selfish, drag, kid, dy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>thanks for lyft credit cannot use cause they d...</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>thanks for lyft credit cannot use cause they d...</td>\n",
       "      <td>[thanks, for, lyft, credit, can, not, use, cau...</td>\n",
       "      <td>[thanks, for, lyft, credit, can, not, use, cau...</td>\n",
       "      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n",
       "      <td>[thank, lyft, credit, use, caus, offer, wheelc...</td>\n",
       "      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "      <td>[bihday, majesti]</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1    0.0  when father is dysfunctional and is so selfish...   \n",
       "1   2    0.0  thanks for lyft credit cannot use cause they d...   \n",
       "2   3    0.0                                bihday your majesty   \n",
       "\n",
       "                                        tweet_source  \\\n",
       "0   @user when a father is dysfunctional and is s...   \n",
       "1  @user @user thanks for #lyft credit i can't us...   \n",
       "2                                bihday your majesty   \n",
       "\n",
       "                                              tweet_  \\\n",
       "0  when father is dysfunctional and is so selfish...   \n",
       "1  thanks for lyft credit cannot use cause they d...   \n",
       "2                                bihday your majesty   \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  [when, father, is, dysfunctional, and, is, so,...   \n",
       "1  [thanks, for, lyft, credit, can, not, use, cau...   \n",
       "2                            [bihday, your, majesty]   \n",
       "\n",
       "                                        tweet_token_  \\\n",
       "0  [when, father, is, dysfunctional, and, is, so,...   \n",
       "1  [thanks, for, lyft, credit, can, not, use, cau...   \n",
       "2                            [bihday, your, majesty]   \n",
       "\n",
       "                                tweet_token_filtered  \\\n",
       "0  [father, dysfunctional, selfish, drags, kids, ...   \n",
       "1  [thanks, lyft, credit, use, cause, offer, whee...   \n",
       "2                                  [bihday, majesty]   \n",
       "\n",
       "                                       tweet_stemmed  \\\n",
       "0  [father, dysfunct, selfish, drag, kid, dysfunc...   \n",
       "1  [thank, lyft, credit, use, caus, offer, wheelc...   \n",
       "2                                  [bihday, majesti]   \n",
       "\n",
       "                                    tweet_lemmatized  \n",
       "0  [father, dysfunctional, selfish, drag, kid, dy...  \n",
       "1  [thanks, lyft, credit, use, cause, offer, whee...  \n",
       "2                                  [bihday, majesty]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('preprocessed_tweets.pickle')\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T21:33:07.363801Z",
     "start_time": "2020-12-05T21:33:07.335803Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49159, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Задание 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Используя библиотеку Spacy, вывести ТОП-20 популярных NER в combine_df датасете. Какой тип NER (ORG, GPE, PERSON и тд) оказался самым популярным? (Учтите, что max_word_limit_spacy для Spacy = 1000000)\n",
    "С помощью Spacy выяснить: какие персоны и организации самые обсуждаемые в train и test датасетах? вывести ТОП-20 самых популярных. Действительно ли в топ вошли только персоны и организации или есть мусор?\n",
    "Повторим шаги из заданий 1 и 2, используя библиотеку nltk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Загружаем предобученную модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T21:33:09.089143Z",
     "start_time": "2020-12-05T21:33:07.369240Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T21:33:09.246832Z",
     "start_time": "2020-12-05T21:33:09.094674Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'tweet': df['tweet_token'][:2500]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T21:33:09.294807Z",
     "start_time": "2020-12-05T21:33:09.254810Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[when, father, is, dysfunctional, and, is, so,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[thanks, for, lyft, credit, can, not, use, cau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet\n",
       "0  [when, father, is, dysfunctional, and, is, so,...\n",
       "1  [thanks, for, lyft, credit, can, not, use, cau...\n",
       "2                            [bihday, your, majesty]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T21:33:43.834310Z",
     "start_time": "2020-12-05T21:33:09.310541Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 34.5 s\n"
     ]
    }
   ],
   "source": [
    "# %time df['ner_labels'] = df['tweet'].apply(lambda row: [word.ent_type_ for word in nlp(' '.join(row))])\n",
    "# %time df['ner_indexes'] = df['ner_labels'].apply(lambda row: [ind for ind in range(len(row)) if row[ind]])\n",
    "%time df['ner'] = df['tweet'].apply(lambda row: [(word.ent_type_, word.text) for word in nlp(' '.join(row)) if \\\n",
    "                                                 word.ent_type_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T21:33:43.881026Z",
     "start_time": "2020-12-05T21:33:43.839027Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>ner</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[when, father, is, dysfunctional, and, is, so,...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[thanks, for, lyft, credit, can, not, use, cau...</td>\n",
       "      <td>[(ORG, wheelchair), (ORG, vans), (ORG, pdx)]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "      <td>[(PERSON, bihday)]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  \\\n",
       "0  [when, father, is, dysfunctional, and, is, so,...   \n",
       "1  [thanks, for, lyft, credit, can, not, use, cau...   \n",
       "2                            [bihday, your, majesty]   \n",
       "\n",
       "                                            ner  \n",
       "0                                            []  \n",
       "1  [(ORG, wheelchair), (ORG, vans), (ORG, pdx)]  \n",
       "2                            [(PERSON, bihday)]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "TOP-20 популярных NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T21:33:44.597057Z",
     "start_time": "2020-12-05T21:33:43.887026Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "ner_list = df['ner'].sum()\n",
    "ner_count_dict = {}\n",
    "\n",
    "for ner in ner_list:\n",
    "    ner_count_dict[ner] = ner_list.count(ner)\n",
    "    \n",
    "ner_count_dict_sorted = sorted(list(ner_count_dict.items()), key=lambda val: val[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T21:33:44.613024Z",
     "start_time": "2020-12-05T21:33:44.600025Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('DATE', 'today'), 94),\n",
       " (('DATE', 'day'), 60),\n",
       " (('DATE', 'the'), 47),\n",
       " (('DATE', 'weekend'), 37),\n",
       " (('CARDINAL', 'one'), 37)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_count_dict_sorted[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T21:33:44.661022Z",
     "start_time": "2020-12-05T21:33:44.621023Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('ORG', 'am'), 34),\n",
       " (('PERSON', 'bihday'), 26),\n",
       " (('GPE', 'orlando'), 18),\n",
       " (('GPE', 'america'), 12),\n",
       " (('PERSON', 'hu'), 11),\n",
       " (('PERSON', 'hea'), 9),\n",
       " (('ORG', 'house'), 9),\n",
       " (('GPE', 'london'), 8),\n",
       " (('PERSON', 'christina'), 7),\n",
       " (('GPE', 'florida'), 7),\n",
       " (('ORG', 'bong'), 7),\n",
       " (('ORG', 'bing'), 7),\n",
       " (('ORG', 'euro'), 6),\n",
       " (('GPE', 'japan'), 6),\n",
       " (('PERSON', 'hillary'), 6),\n",
       " (('GPE', 'libtard'), 6),\n",
       " (('GPE', 'sjw'), 6),\n",
       " (('GPE', 'us'), 6),\n",
       " (('ORG', 'gon'), 6),\n",
       " (('ORG', 'bull'), 6)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_count_dict_sorted = [val for val in ner_count_dict_sorted if val[0][0] in ['ORG', 'GPE', 'PERSON']]\n",
    "\n",
    "ner_count_dict_sorted[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Какой тип NER (ORG, GPE, PERSON и тд) оказался самым популярным?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T21:33:44.709027Z",
     "start_time": "2020-12-05T21:33:44.668027Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORG 613\n",
      "PERSON 880\n",
      "PGE 258\n",
      "\n",
      "Тип PERSON самый популярный в датасете.\n"
     ]
    }
   ],
   "source": [
    "org_count = []\n",
    "person_count = []\n",
    "gpe_count = []\n",
    "\n",
    "[org_count.append(val[1]) for val in ner_count_dict_sorted if val[0][0] == 'ORG']\n",
    "[person_count.append(val[1]) for val in ner_count_dict_sorted if val[0][0] == 'PERSON']\n",
    "[gpe_count.append(val[1]) for val in ner_count_dict_sorted if val[0][0] == 'GPE']\n",
    "\n",
    "print('ORG', sum(org_count))\n",
    "print('PERSON', sum(person_count))\n",
    "print('PGE', sum(gpe_count))\n",
    "\n",
    "print('\\nТип PERSON самый популярный в датасете.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "С помощью Spacy выяснить: какие персоны и организации самые обсуждаемые в train и test датасетах? вывести ТОП-20 самых популярных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T21:33:44.741035Z",
     "start_time": "2020-12-05T21:33:44.715026Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['am', 'house', 'bong', 'bing', 'euro', 'gon', 'bull', 'positive', 'sun', 'buffalo', 'school', 'nba', 'orlando', 'usa', 'app', 'time', 'sma', 'islam', 'hill', 'social']\n",
      "\n",
      "['bihday', 'hu', 'hea', 'christina', 'hillary', 'grimmie', 'sun', 'dance', 'dog', 'ff', 'cat', 'warcraft', 'mm', 'browser', 'tom', 'marbs', 'lil', 'hope', 'blur', 'am']\n"
     ]
    }
   ],
   "source": [
    "print([val[0][1] for val in ner_count_dict_sorted if val[0][0] == 'ORG'][:20])\n",
    "print()\n",
    "print([val[0][1] for val in ner_count_dict_sorted if val[0][0] == 'PERSON'][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Действительно ли в топ вошли только персоны и организации или есть мусор?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "В ТОП'ах в разрезе типов именованных сущностей приличная доля мусора."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Задание 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Используя библиотеку nltk, вывести ТОП-20 популярных NER в combine_df датасете. Какой тип NER (ORG, GPE, PERSON и тд) оказался самым популярным? Для данного задания используем ограничение на количество символов во входном датасете (max_word_limit_spacy = 1000000), чтобы иметь возможность сравнить результаты работы Spacy и nltk. Обратите внимание, что nltk чувствителен к регистру.\n",
    "С помощью nltk выяснить: какие персоны и организации самые обсуждаемые в train и test датасетах? вывести ТОП-20 самых популярных. Действительно ли в топ вошли только персоны и организации или есть мусор?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T21:33:45.911031Z",
     "start_time": "2020-12-05T21:33:44.747025Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[when, father, is, dysfunctional, and, is, so,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[thanks, for, lyft, credit, can, not, use, cau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet\n",
       "0  [when, father, is, dysfunctional, and, is, so,...\n",
       "1  [thanks, for, lyft, credit, can, not, use, cau...\n",
       "2                            [bihday, your, majesty]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('preprocessed_tweets.pickle')\n",
    "\n",
    "df = pd.DataFrame({'tweet': df['tweet_token_'][:1250]})\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T21:33:45.927107Z",
     "start_time": "2020-12-05T21:33:45.916499Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['trump', 'calling', 'on', 'obama', 'to', 'resign', 'over', 'the', 'orlando', 'shootings', 'the', 'boy', 'has', 'point', 'years', 'and', 'all', 'he', 'did', 'was', 'talk', 'about', 'change']\n"
     ]
    }
   ],
   "source": [
    "for val in df['tweet'].values:\n",
    "    if 'trump' in val:\n",
    "        print(val)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T21:33:46.053056Z",
     "start_time": "2020-12-05T21:33:45.932231Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_ = pd.read_csv('train_tweets.csv')\n",
    "\n",
    "df_.head()\n",
    "\n",
    "for val in df_['tweet'].values:\n",
    "    if 'Trump' in val:\n",
    "        print('Trump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T21:33:46.085023Z",
     "start_time": "2020-12-05T21:33:46.057023Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Как оказалось, NLTK не воспринимает слова с маленькой буквы как NE. Видимо это обязательное условие чтобы модуль хотя бы начал рассматривать токен в качестве кандидата на NE. Ниже эксперимент. У нас не только в предобработанном дата-сете слова переведены в нижний регистр, но и в исходных данных все слова в нижнем регистре изначально, поэтому предобработать данные с сохранением регистра символов нет возможности. Возможно, в NLTK NER есть акакой-то режим, позволяющий игнорировать это ограничение - пока такого не нашел."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T21:33:56.707650Z",
     "start_time": "2020-12-05T21:33:46.090574Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for ind in range(1000):\n",
    "    tw = [('trump' if word == 'trump' else word) for word in df['tweet'].values[ind]]\n",
    "    tw = ' '.join(tw)\n",
    "\n",
    "    for val in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(tw))):\n",
    "        if hasattr(val, 'label'):\n",
    "            print(val.label(), val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T21:34:06.116541Z",
     "start_time": "2020-12-05T21:33:56.711550Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPE (GPE Trump/NNP)\n",
      "PERSON (PERSON Trump/NNP)\n",
      "PERSON (PERSON Trump/NNP)\n",
      "PERSON (PERSON Trump/NNP)\n",
      "PERSON (PERSON Trump/NNP)\n",
      "PERSON (PERSON Trump/NNP)\n",
      "GPE (GPE Trump/NN)\n",
      "PERSON (PERSON Trump/NNP)\n",
      "PERSON (PERSON Trump/NNP)\n",
      "PERSON (PERSON Trump/NNP)\n",
      "PERSON (PERSON Trump/NNP)\n"
     ]
    }
   ],
   "source": [
    "for ind in range(1000):\n",
    "    tw = [('Trump' if word == 'trump' else word) for word in df['tweet'].values[ind]]\n",
    "    tw = ' '.join(tw)\n",
    "\n",
    "    for val in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(tw))):\n",
    "        if hasattr(val, 'label'):\n",
    "            print(val.label(), val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### Задание 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Какая из библиотек по вашему лучше отработала? Сравните качество полученных most_common NER и количество распознаных NER.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Ну, поскольку с помощью NLTK получилось в принципе извлечь NE из корпуса, spacy отработал, конечно, получше, хотя не с идеальным качеством."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тема “Улучшение качества  POS-tagger'а”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "На вебинаре был рассмотрен пример тегирования с использованием корпуса на Русском языке, вам необходимо улучшить модель что бы качество классификации было выше чем с лог регрессией "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T21:37:28.856509Z",
     "start_time": "2020-12-05T21:37:28.649512Z"
    },
    "hidden": true
   },
   "source": [
    "!wget -O ./datasets/ru_syntagrus-ud-train.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-train.conllu\n",
    "!wget -O ./datasets/ru_syntagrus-ud-dev.conllu https://raw.githubusercontent.com/UniversalDependencies/UD_Russian-SynTagRus/master/ru_syntagrus-ud-dev.conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T21:49:19.196891Z",
     "start_time": "2020-12-05T21:48:42.504069Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "full_train = pyconll.load_from_file('datasets/ru_syntagrus-ud-train.conllu')\n",
    "full_test = pyconll.load_from_file('datasets/ru_syntagrus-ud-dev.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T21:49:21.176997Z",
     "start_time": "2020-12-05T21:49:19.202509Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fdata_train = []\n",
    "for sent in full_train[:]:\n",
    "    fdata_train.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_test.append([(token.form, token.upos) for token in sent])\n",
    "    \n",
    "fdata_sent_test = []\n",
    "for sent in full_test[:]:\n",
    "    fdata_sent_test.append([token.form for token in sent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T21:54:21.435565Z",
     "start_time": "2020-12-05T21:54:20.191989Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Анкета .\n",
      "Начальник областного управления связи Семен Еремеевич был человек простой , приходил на работу всегда вовремя , здоровался с секретаршей за руку и иногда даже писал в стенгазету заметки под псевдонимом \" Муха \" .\n",
      "В приемной его с утра ожидали посетители , - кое-кто с важными делами , а кое-кто и с такими , которые легко можно было решить в нижестоящих инстанциях , не затрудняя Семена Еремеевича .\n",
      "Однако стиль работы Семена Еремеевича заключался в том , чтобы принимать всех желающих и лично вникать в дело .\n",
      "Приемная была обставлена просто , но по-деловому .\n",
      "У двери стоял стол секретарши , на столе - пишущая машинка с широкой кареткой .\n",
      "В углу висел репродуктор и играло радио для развлечения ожидающих и еще для того , чтобы заглушать голос начальника , доносившийся из кабинета , так как , бесспорно , среди посетителей могли находиться и случайные люди .\n",
      "Кабинет отличался скромностью , присущей Семену Еремеевичу .\n",
      "В глубине стоял широкий письменный стол с бронзовыми чернильницами и перед ним два кожаных кресла .\n",
      "Справа был стол для заседаний - длинный , накрытый зеленым сукном и с обеих сторон аккуратно заставленный стульями .\n"
     ]
    }
   ],
   "source": [
    "all_train_texts = [' '.join(token.form for token in sent) for sent in full_train]\n",
    "all_test_texts = [' '.join(token.form for token in sent) for sent in full_test]\n",
    "\n",
    "all_train_labels = [' '.join(token.form for token in sent) for sent in full_train]\n",
    "all_test_labels = [' '.join(token.form for token in sent) for sent in full_test]\n",
    "print('\\n'.join(all_train_texts[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T21:54:22.340640Z",
     "start_time": "2020-12-05T21:54:21.439561Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_tok = []\n",
    "train_label = []\n",
    "for sent in fdata_train[:]:\n",
    "    for tok in sent:\n",
    "        train_tok.append(tok[0])\n",
    "        train_label.append('NO_TAG' if tok[1] is None else tok[1])\n",
    "        \n",
    "test_tok = []\n",
    "test_label = []\n",
    "for sent in fdata_test[:]:\n",
    "    for tok in sent:\n",
    "        test_tok.append(tok[0])\n",
    "        test_label.append('NO_TAG' if tok[1] is None else tok[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T21:54:23.622954Z",
     "start_time": "2020-12-05T21:54:22.345676Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "train_enc_labels = le.fit_transform(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T21:54:23.904040Z",
     "start_time": "2020-12-05T21:54:23.627850Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_enc_labels = le.transform(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T21:54:23.934844Z",
     "start_time": "2020-12-05T21:54:23.912844Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN',\n",
       "       'NO_TAG', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM',\n",
       "       'VERB', 'X'], dtype='<U6')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T21:54:23.965843Z",
     "start_time": "2020-12-05T21:54:23.944844Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hvectorizer = HashingVectorizer(ngram_range=(1, 3), analyzer='char', n_features=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T21:54:37.985939Z",
     "start_time": "2020-12-05T21:54:23.974852Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train = hvectorizer.fit_transform(train_tok)\n",
    "X_test = hvectorizer.transform(test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T21:54:39.944079Z",
     "start_time": "2020-12-05T21:54:39.920846Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(871526, 50)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T21:57:47.234612Z",
     "start_time": "2020-12-05T21:54:39.949849Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mihail\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Mihail\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\Users\\Mihail\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=10,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=0, max_iter=10)\n",
    "lr.fit(X_train, train_enc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T21:57:47.343708Z",
     "start_time": "2020-12-05T21:57:47.239578Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T21:57:47.390610Z",
     "start_time": "2020-12-05T21:57:47.351577Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6331092238735551"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_enc_labels, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "accuracy_score логистической регрессии: 0.63310"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### Улучшенное решение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T22:14:42.998588Z",
     "start_time": "2020-12-05T22:14:42.979530Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hvectorizer = HashingVectorizer(ngram_range=(1, 3), analyzer='char', n_features=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T22:14:59.463949Z",
     "start_time": "2020-12-05T22:14:43.009597Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X_train = hvectorizer.fit_transform(train_tok)\n",
    "X_test = hvectorizer.transform(test_tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T22:18:40.957999Z",
     "start_time": "2020-12-05T22:14:59.471921Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=10,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=0, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=0, max_iter=10)\n",
    "lr.fit(X_train, train_enc_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-05T22:18:41.114197Z",
     "start_time": "2020-12-05T22:18:40.965195Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.700392612812995"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = lr.predict(X_test)\n",
    "accuracy_score(test_enc_labels, pred)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "notify_time": "30",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Menu",
   "title_sidebar": "Menu",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
