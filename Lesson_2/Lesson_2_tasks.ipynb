{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T12:39:41.113895Z",
     "start_time": "2020-12-01T12:39:36.324479Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создайте мешок слов с помощью sklearn.feature_extraction.text.CountVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.\n",
    "*\tИгнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.\n",
    "*\tОграничим количество слов, попадающий в мешок, с помощью max_features = 1000.\n",
    "*\tИсключим стоп-слова с помощью stop_words='english'.\n",
    "*\tОтобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью CountVectorizer.get_feature_names()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T12:39:42.267025Z",
     "start_time": "2020-12-01T12:39:41.121096Z"
    }
   },
   "outputs": [],
   "source": [
    "os.listdir()\n",
    "\n",
    "with open('preprocessed_tweets.pickle', 'rb') as f:\n",
    "    preprocessed_tweets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T12:39:42.345799Z",
     "start_time": "2020-12-01T12:39:42.281800Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49159, 8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T12:39:42.470910Z",
     "start_time": "2020-12-01T12:39:42.353797Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_token_filtered</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_source</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>when father is dysfunctional and is so selfish...</td>\n",
       "      <td>[when, father, is, dysfunctional, and, is, so,...</td>\n",
       "      <td>[father, dysfunctional, selfish, drags, kids, ...</td>\n",
       "      <td>[father, dysfunct, selfish, drag, kid, dysfunc...</td>\n",
       "      <td>when father is dysfunctional and is so selfish...</td>\n",
       "      <td>[father, dysfunctional, selfish, drag, kid, dy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>thanks for lyft credit cannot use cause they d...</td>\n",
       "      <td>[thanks, for, lyft, credit, can, not, use, cau...</td>\n",
       "      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n",
       "      <td>[thank, lyft, credit, use, caus, offer, wheelc...</td>\n",
       "      <td>thanks for lyft credit cannot use cause they d...</td>\n",
       "      <td>[thanks, lyft, credit, use, cause, offer, whee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>[bihday, your, majesty]</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "      <td>[bihday, majesti]</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>[bihday, majesty]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet  \\\n",
       "0   1    0.0  when father is dysfunctional and is so selfish...   \n",
       "1   2    0.0  thanks for lyft credit cannot use cause they d...   \n",
       "2   3    0.0                                bihday your majesty   \n",
       "\n",
       "                                         tweet_token  \\\n",
       "0  [when, father, is, dysfunctional, and, is, so,...   \n",
       "1  [thanks, for, lyft, credit, can, not, use, cau...   \n",
       "2                            [bihday, your, majesty]   \n",
       "\n",
       "                                tweet_token_filtered  \\\n",
       "0  [father, dysfunctional, selfish, drags, kids, ...   \n",
       "1  [thanks, lyft, credit, use, cause, offer, whee...   \n",
       "2                                  [bihday, majesty]   \n",
       "\n",
       "                                       tweet_stemmed  \\\n",
       "0  [father, dysfunct, selfish, drag, kid, dysfunc...   \n",
       "1  [thank, lyft, credit, use, caus, offer, wheelc...   \n",
       "2                                  [bihday, majesti]   \n",
       "\n",
       "                                        tweet_source  \\\n",
       "0  when father is dysfunctional and is so selfish...   \n",
       "1  thanks for lyft credit cannot use cause they d...   \n",
       "2                                bihday your majesty   \n",
       "\n",
       "                                    tweet_lemmatized  \n",
       "0  [father, dysfunctional, selfish, drag, kid, dy...  \n",
       "1  [thanks, lyft, credit, use, cause, offer, whee...  \n",
       "2                                  [bihday, majesty]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_tweets.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T12:39:42.502799Z",
     "start_time": "2020-12-01T12:39:42.479798Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_tweets = preprocessed_tweets[:10000].copy()\n",
    "()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T12:39:42.549797Z",
     "start_time": "2020-12-01T12:39:42.509801Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 1),\n",
    "                             analyzer='word',\n",
    "                             max_features=1000,\n",
    "                             max_df=0.9,\n",
    "                             stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T12:39:44.289011Z",
     "start_time": "2020-12-01T12:39:42.563799Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_bag_of_words_by_column_name(column, vectorizer):\n",
    "    tokens = []\n",
    "    [[tokens.append(token) for token in list_] for list_ in preprocessed_tweets[column].values]\n",
    "\n",
    "    bag_of_words = vectorizer.fit_transform(tokens)\n",
    "    return bag_of_words, vectorizer.get_feature_names()\n",
    "\n",
    "bag_of_words_stemmed, feature_names_stemmed = get_bag_of_words_by_column_name('tweet_stemmed', vectorizer)\n",
    "bag_of_words_lemmatized, feature_names_lemmatized = get_bag_of_words_by_column_name('tweet_lemmatized', vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T12:39:44.587802Z",
     "start_time": "2020-12-01T12:39:44.293796Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aap</th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accept</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actual</th>\n",
       "      <th>ad</th>\n",
       "      <th>...</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>young</th>\n",
       "      <th>youth</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aap  abl  absolut  accept  account  act  action  actor  actual  ad  ...  \\\n",
       "0    0    0        0       0        0    0       0      0       0   0  ...   \n",
       "1    0    0        0       0        0    0       0      0       0   0  ...   \n",
       "2    0    0        0       0        0    0       0      0       0   0  ...   \n",
       "3    0    0        0       0        0    0       0      0       0   0  ...   \n",
       "4    0    0        0       0        0    0       0      0       0   0  ...   \n",
       "\n",
       "   yeah  year  yesterday  yo  yoga  young  youth  youtub  yr  yummi  \n",
       "0     0     0          0   0     0      0      0       0   0      0  \n",
       "1     0     0          0   0     0      0      0       0   0      0  \n",
       "2     0     0          0   0     0      0      0       0   0      0  \n",
       "3     0     0          0   0     0      0      0       0   0      0  \n",
       "4     0     0          0   0     0      0      0       0   0      0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_stemmed = pd.DataFrame(bag_of_words_stemmed.toarray(), columns=feature_names_stemmed)\n",
    "\n",
    "bag_of_words_stemmed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T12:39:44.918798Z",
     "start_time": "2020-12-01T12:39:44.602798Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aap</th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actually</th>\n",
       "      <th>adapt</th>\n",
       "      <th>add</th>\n",
       "      <th>...</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>young</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aap  able  absolutely  account  act  action  actor  actually  adapt  add  \\\n",
       "0    0     0           0        0    0       0      0         0      0    0   \n",
       "1    0     0           0        0    0       0      0         0      0    0   \n",
       "2    0     0           0        0    0       0      0         0      0    0   \n",
       "3    0     0           0        0    0       0      0         0      0    0   \n",
       "4    0     0           0        0    0       0      0         0      0    0   \n",
       "\n",
       "   ...  yeah  year  yes  yesterday  yo  yoga  young  youtube  yr  yummy  \n",
       "0  ...     0     0    0          0   0     0      0        0   0      0  \n",
       "1  ...     0     0    0          0   0     0      0        0   0      0  \n",
       "2  ...     0     0    0          0   0     0      0        0   0      0  \n",
       "3  ...     0     0    0          0   0     0      0        0   0      0  \n",
       "4  ...     0     0    0          0   0     0      0        0   0      0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_lemmatized = pd.DataFrame(bag_of_words_lemmatized.toarray(), columns=feature_names_lemmatized)\n",
    "\n",
    "bag_of_words_lemmatized.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создайте мешок слов с помощью sklearn.feature_extraction.text.TfidfVectorizer.fit_transform(). Применим его к 'tweet_stemmed' и 'tweet_lemmatized' отдельно.\n",
    "*\tИгнорируем слова, частота которых в документе строго превышает порог 0.9 с помощью max_df.\n",
    "*\tОграничим количество слов, попадающий в мешок, с помощью max_features = 1000.\n",
    "*\tИсключим стоп-слова с помощью stop_words='english'.\n",
    "*\tОтобразим Bag-of-Words модель как DataFrame. columns необходимо извлечь с помощью TfidfVectorizer.get_feature_names()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T12:39:44.963797Z",
     "start_time": "2020-12-01T12:39:44.931799Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1),\n",
    "                             analyzer='word',\n",
    "                             max_features=1000,\n",
    "                             max_df=0.9,\n",
    "                             stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T12:39:46.762139Z",
     "start_time": "2020-12-01T12:39:44.969798Z"
    }
   },
   "outputs": [],
   "source": [
    "bag_of_words_stemmed, feature_names_stemmed = get_bag_of_words_by_column_name('tweet_stemmed', vectorizer)\n",
    "bag_of_words_lemmatized, feature_names_lemmatized = get_bag_of_words_by_column_name('tweet_lemmatized', vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T12:39:47.173799Z",
     "start_time": "2020-12-01T12:39:46.769803Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aap</th>\n",
       "      <th>abl</th>\n",
       "      <th>absolut</th>\n",
       "      <th>accept</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actual</th>\n",
       "      <th>ad</th>\n",
       "      <th>...</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>young</th>\n",
       "      <th>youth</th>\n",
       "      <th>youtub</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aap  abl  absolut  accept  account  act  action  actor  actual   ad  ...  \\\n",
       "0  0.0  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0  ...   \n",
       "1  0.0  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0  ...   \n",
       "2  0.0  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0  ...   \n",
       "3  0.0  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0  ...   \n",
       "4  0.0  0.0      0.0     0.0      0.0  0.0     0.0    0.0     0.0  0.0  ...   \n",
       "\n",
       "   yeah  year  yesterday   yo  yoga  young  youth  youtub   yr  yummi  \n",
       "0   0.0   0.0        0.0  0.0   0.0    0.0    0.0     0.0  0.0    0.0  \n",
       "1   0.0   0.0        0.0  0.0   0.0    0.0    0.0     0.0  0.0    0.0  \n",
       "2   0.0   0.0        0.0  0.0   0.0    0.0    0.0     0.0  0.0    0.0  \n",
       "3   0.0   0.0        0.0  0.0   0.0    0.0    0.0     0.0  0.0    0.0  \n",
       "4   0.0   0.0        0.0  0.0   0.0    0.0    0.0     0.0  0.0    0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_stemmed = pd.DataFrame(bag_of_words_stemmed.toarray(), columns=feature_names_stemmed)\n",
    "\n",
    "bag_of_words_stemmed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T12:39:47.519886Z",
     "start_time": "2020-12-01T12:39:47.183236Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aap</th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actually</th>\n",
       "      <th>adapt</th>\n",
       "      <th>add</th>\n",
       "      <th>...</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>yo</th>\n",
       "      <th>yoga</th>\n",
       "      <th>young</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yr</th>\n",
       "      <th>yummy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aap  able  absolutely  account  act  action  actor  actually  adapt  add  \\\n",
       "0  0.0   0.0         0.0      0.0  0.0     0.0    0.0       0.0    0.0  0.0   \n",
       "1  0.0   0.0         0.0      0.0  0.0     0.0    0.0       0.0    0.0  0.0   \n",
       "2  0.0   0.0         0.0      0.0  0.0     0.0    0.0       0.0    0.0  0.0   \n",
       "3  0.0   0.0         0.0      0.0  0.0     0.0    0.0       0.0    0.0  0.0   \n",
       "4  0.0   0.0         0.0      0.0  0.0     0.0    0.0       0.0    0.0  0.0   \n",
       "\n",
       "   ...  yeah  year  yes  yesterday   yo  yoga  young  youtube   yr  yummy  \n",
       "0  ...   0.0   0.0  0.0        0.0  0.0   0.0    0.0      0.0  0.0    0.0  \n",
       "1  ...   0.0   0.0  0.0        0.0  0.0   0.0    0.0      0.0  0.0    0.0  \n",
       "2  ...   0.0   0.0  0.0        0.0  0.0   0.0    0.0      0.0  0.0    0.0  \n",
       "3  ...   0.0   0.0  0.0        0.0  0.0   0.0    0.0      0.0  0.0    0.0  \n",
       "4  ...   0.0   0.0  0.0        0.0  0.0   0.0    0.0      0.0  0.0    0.0  \n",
       "\n",
       "[5 rows x 1000 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_lemmatized = pd.DataFrame(bag_of_words_lemmatized.toarray(), columns=feature_names_lemmatized)\n",
    "\n",
    "bag_of_words_lemmatized.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Натренируем gensim.models.Word2Vec модель на наших данных.\n",
    "*\tТренировать будем на токенизированных твитах combine_df['tweet_token']\n",
    "*\tУстановим следующие параметры: size=200, window=5, min_count=2, sg = 1, hs = 0, negative = 10, workers= 32, seed = 34.\n",
    "*\tИспользуем функцию train() с параметром total_examples равным длине combine_df['tweet_token'], количество epochs установим 20.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T12:40:17.733109Z",
     "start_time": "2020-12-01T12:39:47.529816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20.7 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1748789, 2401300)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelW2V = Word2Vec(sentences=preprocessed_tweets['tweet_token'],\n",
    "                    size=200, \n",
    "                    window=5,\n",
    "                    min_count=2,\n",
    "                    sg = 1,\n",
    "                    hs = 0,\n",
    "                    negative = 10,\n",
    "                    workers= 32,\n",
    "                    seed = 34)\n",
    "\n",
    "%time modelW2V.train(sentences=preprocessed_tweets['tweet_token'], total_examples=modelW2V.corpus_count, epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте немного потестируем нашу модель Word2Vec и посмотрим, как она работает. Мы зададим слово positive = \"dinner\", и модель вытащит из корпуса наиболее похожие слова c помощью функции most_similar. То же самое попробуем со словом \"trump\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T12:40:17.780264Z",
     "start_time": "2020-12-01T12:40:17.738664Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('yum', 0.7131447792053223),\n",
       " ('cuddles', 0.703732430934906),\n",
       " ('celebrations', 0.7027802467346191),\n",
       " ('burgers', 0.6971790194511414),\n",
       " ('adventures', 0.6968140602111816),\n",
       " ('spaghetti', 0.692824125289917),\n",
       " ('delicious', 0.6906303763389587),\n",
       " ('indonesia', 0.6883400082588196),\n",
       " ('goodfriends', 0.6876944303512573),\n",
       " ('sizzle', 0.6870667934417725)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelW2V.wv.most_similar(positive=['dinner'], negative=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T12:40:17.812246Z",
     "start_time": "2020-12-01T12:40:17.788187Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('paladino', 0.6132089495658875),\n",
       " ('sentence', 0.6101824045181274),\n",
       " ('fuhered', 0.598522424697876),\n",
       " ('makeamericagreatagain', 0.5952212810516357),\n",
       " ('michelle', 0.5921273827552795),\n",
       " ('ally', 0.5892149209976196),\n",
       " ('carl', 0.5861276388168335),\n",
       " ('republican', 0.5853856801986694),\n",
       " ('bigot', 0.5835742950439453),\n",
       " ('obama', 0.5809466242790222)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelW2V.wv.most_similar(positive=['trump'], negative=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из приведенных выше примеров мы видим, что наша модель word2vec хорошо справляется с поиском наиболее похожих слов для данного слова. Но как она это делает? Она изучила векторы для каждого уникального слова наших данных и использует косинусное сходство, чтобы найти наиболее похожие векторы (слова).\n",
    "Давайте проверим векторное представление любого слова из нашего корпуса, например \"food\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T12:40:17.875118Z",
     "start_time": "2020-12-01T12:40:17.820377Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.14039822, -0.27341405, -0.04340313,  0.12114415,  0.1608433 ,\n",
       "       -0.40633652, -0.166633  ,  0.45642924, -0.11788867,  0.32769233,\n",
       "       -0.0973983 ,  0.01997017, -0.02217687, -0.2689961 , -0.11332266,\n",
       "        0.2684986 ,  0.23946719,  0.27677667,  0.096652  , -0.03087369,\n",
       "       -0.04402805,  0.20057318, -0.255175  ,  0.08254629,  0.16387029,\n",
       "        0.759173  , -0.10752366,  0.50375724,  0.2783297 , -0.28180802,\n",
       "       -0.18812907, -0.36689538, -0.2009986 , -0.17646085, -0.47613335,\n",
       "        0.49207067,  0.08936849, -0.5008136 ,  0.0801122 , -0.44623265,\n",
       "       -0.39515212, -0.11351427, -0.20132364, -0.35673624, -0.03771339,\n",
       "       -0.18257959, -0.13106164,  0.396421  , -0.23522   , -0.48553345,\n",
       "        0.31243217, -0.11469059, -0.01180932, -0.11669469, -0.17518467,\n",
       "       -0.09934748,  0.32758036,  0.7459076 ,  0.38597414,  0.45526016,\n",
       "       -0.08103848, -0.04867301,  0.29034093,  0.25527358,  0.4563406 ,\n",
       "       -0.29469383,  0.4956431 , -0.23957852, -0.06377281,  0.06723598,\n",
       "       -0.20599504,  0.46508226,  0.08574135, -0.2636121 ,  0.07598982,\n",
       "       -0.37104782, -0.05016856, -0.28640875,  0.09757171, -0.10820913,\n",
       "        0.50334316, -0.0624909 , -0.05231649,  0.3912599 ,  0.15500107,\n",
       "       -0.04937223,  0.14353399,  0.5978095 , -0.22508441,  0.02210136,\n",
       "       -0.57088643, -0.35858396, -0.22800635, -0.32417244, -0.07457284,\n",
       "       -0.20494734, -0.83703357, -0.03412622, -0.03891746,  0.6032005 ,\n",
       "        0.02432798,  0.2573815 , -0.14151728,  0.3236756 , -0.01096607,\n",
       "       -0.28872854,  0.0614305 , -0.09460238,  0.5603685 ,  0.38380307,\n",
       "        0.08023899, -0.14807464, -0.62788576, -0.18714546, -0.23202439,\n",
       "        0.67517906,  0.22562814, -0.22588   ,  0.09420427,  0.18691681,\n",
       "        0.08099965, -0.41836005, -0.16182728,  0.631534  , -0.01257808,\n",
       "       -0.5764636 ,  0.94987285,  0.20383953, -0.39951465, -0.28781334,\n",
       "       -0.13342217,  0.4302902 , -0.00803023,  0.02862094, -0.6191644 ,\n",
       "       -0.6875248 ,  0.08813982,  0.44840765,  0.23132826, -0.22806048,\n",
       "        0.26248205, -0.37386197, -0.13673757,  0.1517657 ,  0.07689165,\n",
       "        0.27974626,  0.28161156,  0.32390925,  0.55124664, -0.23280746,\n",
       "       -0.29965252,  0.17957844,  0.5187149 , -0.10925158, -1.0629404 ,\n",
       "        0.26560488, -0.09380255,  0.07160604, -0.40813142, -0.34610322,\n",
       "        0.27208737,  0.06048756, -0.15891397, -0.1294487 , -0.55253464,\n",
       "       -0.08241314, -0.24610762,  0.09890042,  0.57121676, -0.38782424,\n",
       "        0.3358569 , -0.11968596, -0.37543046, -0.5037728 ,  0.43595308,\n",
       "       -0.4531151 ,  0.36045948, -0.23724346, -0.1549658 ,  0.45030743,\n",
       "       -0.41436756,  0.08673695,  0.36282864, -0.5109841 ,  0.04083513,\n",
       "       -0.1329978 , -0.22977123,  0.28995952, -0.16284624,  0.0663486 ,\n",
       "        0.47198167,  0.00330406, -0.31729245,  0.30657023,  0.18152145,\n",
       "        0.06304712, -0.2942708 ,  0.05337837,  0.1395308 , -0.17536628],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelW2V.wv['food']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task_6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку наши данные содержат твиты, а не только слова, нам придется придумать способ использовать векторы слов из модели word2vec для создания векторного представления всего твита. Существует простое решение этой проблемы, мы можем просто взять среднее значение всех векторов слов, присутствующих в твите. Длина результирующего вектора будет одинаковой, то есть 200. Мы повторим тот же процесс для всех твитов в наших данных и получим их векторы. Теперь у нас есть 200 функций word2vec для наших данных.\n",
    "Необходимо создать вектор для каждого твита, взяв среднее значение векторов слов, присутствующих в твите. В цикле сделать:  vec += model_w2v[word].reshape((1, size))\n",
    "и поделить финальный вектор на количество слов в твите.\n",
    "На выходе должен получиться wordvec_df.shape = (49159, 200).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T12:40:20.659041Z",
     "start_time": "2020-12-01T12:40:17.913114Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mihail\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in true_divide\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>190</th>\n",
       "      <th>191</th>\n",
       "      <th>192</th>\n",
       "      <th>193</th>\n",
       "      <th>194</th>\n",
       "      <th>195</th>\n",
       "      <th>196</th>\n",
       "      <th>197</th>\n",
       "      <th>198</th>\n",
       "      <th>199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.083529</td>\n",
       "      <td>0.186651</td>\n",
       "      <td>-0.151521</td>\n",
       "      <td>0.179136</td>\n",
       "      <td>0.145685</td>\n",
       "      <td>-0.058510</td>\n",
       "      <td>-0.213681</td>\n",
       "      <td>0.349438</td>\n",
       "      <td>-0.059500</td>\n",
       "      <td>0.061580</td>\n",
       "      <td>...</td>\n",
       "      <td>0.334167</td>\n",
       "      <td>-0.077468</td>\n",
       "      <td>0.033498</td>\n",
       "      <td>0.050663</td>\n",
       "      <td>0.061478</td>\n",
       "      <td>0.193505</td>\n",
       "      <td>-0.077190</td>\n",
       "      <td>0.024321</td>\n",
       "      <td>-0.089858</td>\n",
       "      <td>-0.133916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.066976</td>\n",
       "      <td>0.085508</td>\n",
       "      <td>-0.118966</td>\n",
       "      <td>0.117939</td>\n",
       "      <td>0.056686</td>\n",
       "      <td>0.039291</td>\n",
       "      <td>-0.202806</td>\n",
       "      <td>0.227319</td>\n",
       "      <td>0.117310</td>\n",
       "      <td>0.089611</td>\n",
       "      <td>...</td>\n",
       "      <td>0.342863</td>\n",
       "      <td>-0.022509</td>\n",
       "      <td>-0.033464</td>\n",
       "      <td>-0.080543</td>\n",
       "      <td>0.134408</td>\n",
       "      <td>0.306458</td>\n",
       "      <td>-0.130963</td>\n",
       "      <td>-0.033288</td>\n",
       "      <td>-0.124975</td>\n",
       "      <td>0.036728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.051177</td>\n",
       "      <td>0.338907</td>\n",
       "      <td>-0.148833</td>\n",
       "      <td>-0.245586</td>\n",
       "      <td>0.384201</td>\n",
       "      <td>-0.377272</td>\n",
       "      <td>-0.114140</td>\n",
       "      <td>0.261264</td>\n",
       "      <td>-0.135387</td>\n",
       "      <td>0.006695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450665</td>\n",
       "      <td>-0.071401</td>\n",
       "      <td>0.275925</td>\n",
       "      <td>0.293299</td>\n",
       "      <td>-0.124057</td>\n",
       "      <td>0.190717</td>\n",
       "      <td>-0.096149</td>\n",
       "      <td>0.162370</td>\n",
       "      <td>0.236093</td>\n",
       "      <td>-0.055047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.295878</td>\n",
       "      <td>0.346695</td>\n",
       "      <td>-0.348219</td>\n",
       "      <td>0.037336</td>\n",
       "      <td>-0.351700</td>\n",
       "      <td>0.146056</td>\n",
       "      <td>-0.168593</td>\n",
       "      <td>0.409233</td>\n",
       "      <td>0.343677</td>\n",
       "      <td>-0.399432</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145061</td>\n",
       "      <td>-0.099427</td>\n",
       "      <td>0.467728</td>\n",
       "      <td>0.217028</td>\n",
       "      <td>0.088860</td>\n",
       "      <td>0.536089</td>\n",
       "      <td>-0.140193</td>\n",
       "      <td>-0.247336</td>\n",
       "      <td>-0.188619</td>\n",
       "      <td>-0.203294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.055379</td>\n",
       "      <td>-0.223584</td>\n",
       "      <td>-0.239282</td>\n",
       "      <td>-0.001742</td>\n",
       "      <td>-0.094719</td>\n",
       "      <td>0.050812</td>\n",
       "      <td>-0.097930</td>\n",
       "      <td>0.218651</td>\n",
       "      <td>0.092532</td>\n",
       "      <td>0.323498</td>\n",
       "      <td>...</td>\n",
       "      <td>0.411779</td>\n",
       "      <td>-0.141490</td>\n",
       "      <td>-0.023522</td>\n",
       "      <td>-0.137062</td>\n",
       "      <td>0.345956</td>\n",
       "      <td>0.175669</td>\n",
       "      <td>0.076918</td>\n",
       "      <td>0.023394</td>\n",
       "      <td>0.065139</td>\n",
       "      <td>-0.002678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6    \\\n",
       "0  0.083529  0.186651 -0.151521  0.179136  0.145685 -0.058510 -0.213681   \n",
       "1 -0.066976  0.085508 -0.118966  0.117939  0.056686  0.039291 -0.202806   \n",
       "2  0.051177  0.338907 -0.148833 -0.245586  0.384201 -0.377272 -0.114140   \n",
       "3  0.295878  0.346695 -0.348219  0.037336 -0.351700  0.146056 -0.168593   \n",
       "4  0.055379 -0.223584 -0.239282 -0.001742 -0.094719  0.050812 -0.097930   \n",
       "\n",
       "        7         8         9    ...       190       191       192       193  \\\n",
       "0  0.349438 -0.059500  0.061580  ...  0.334167 -0.077468  0.033498  0.050663   \n",
       "1  0.227319  0.117310  0.089611  ...  0.342863 -0.022509 -0.033464 -0.080543   \n",
       "2  0.261264 -0.135387  0.006695  ...  0.450665 -0.071401  0.275925  0.293299   \n",
       "3  0.409233  0.343677 -0.399432  ...  0.145061 -0.099427  0.467728  0.217028   \n",
       "4  0.218651  0.092532  0.323498  ...  0.411779 -0.141490 -0.023522 -0.137062   \n",
       "\n",
       "        194       195       196       197       198       199  \n",
       "0  0.061478  0.193505 -0.077190  0.024321 -0.089858 -0.133916  \n",
       "1  0.134408  0.306458 -0.130963 -0.033288 -0.124975  0.036728  \n",
       "2 -0.124057  0.190717 -0.096149  0.162370  0.236093 -0.055047  \n",
       "3  0.088860  0.536089 -0.140193 -0.247336 -0.188619 -0.203294  \n",
       "4  0.345956  0.175669  0.076918  0.023394  0.065139 -0.002678  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_vec_by_tweet(tweet):\n",
    "    vector = np.zeros(200)\n",
    "    words_counter = 0\n",
    "    \n",
    "    for word in tweet:\n",
    "        try:\n",
    "            vector += modelW2V.wv[word]\n",
    "            words_counter += 1\n",
    "        except:\n",
    "            pass\n",
    "    return (vector / words_counter).tolist()\n",
    "\n",
    "wordvec_df = []\n",
    "\n",
    "wordvec_df = pd.DataFrame([get_vec_by_tweet(tweet) for tweet in preprocessed_tweets['tweet_lemmatized']])\n",
    "wordvec_df.head()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Menu",
   "title_sidebar": "Menu",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
